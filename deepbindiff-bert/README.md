# Train BERT From Scratch
## Files for Training BERT
- [`tokenization.py`](./tokenization.py) : Tokenizers adopted from the original Google BERT's code
- [`checkpoint.py`](./checkpoint.py) : Functions to load a model from tensorflow's checkpoint file
- [`models.py`](./models.py) : Model classes for a general transformer
- [`optim.py`](./optim.py) : A custom optimizer (BertAdam class) adopted from Hugging Face's code
- [`train.py`](./train.py) : A helper class for training and evaluation
- [`utils.py`](./utils.py) : Several utility functions
- [`pretrain.py`](./pretrain.py) : An example code for pre-training transformer
- [`classify.py`](./classify.py) : An example code for fine-tuning using pre-trained transformer

## Files for Generating Dictionary and corpus
Since training BERT requires a word dictionary and a given corpus, we create several scripts for generating these files. The generated vocabulary and corpora are stored in `data` with subdirectories named from 0 to the number of pairs under the given path and binary name.
- [`gendata.py`](./gendata.py) : Generate dictionary and walks for a single binary file
- [`dataprocess.py`](./dataprocess.py) : Generate the final dictionary and combined corpus to be fed to BERT by using the files generated by `gendata.py`

## Configuration
### 1. Prepare Experiment Data
> Set up should be done in `gendata.py`.
- `biname` : The binary to be paired
- `path` : Root directory of binaries
- `outputDir` : Directory for storing temporary data (not useful in this task). 

In order to generate a fine-grained corpus from raw data, we should first prepare enough binary paires (> 100 pairs is recommended). Just for example, we pair all the chroot binary of coreutils and store part of the graph merging results in directory `data`.

### 2. Generate Corpus and Vocabulary
> Set up should be done in `dataprocess.py`.
- `vocab/vocab.txt` : Generated vocabulary
- `data/corpus.txt` : Generated corpus

### 3. Configure BERT Settings
> Set up should be done in `config/pretrain.json`.
- `seed` : random seed
- `batch_size` : size of batch in a single step for training. We need a smaller size of batch if GPU memory is limited.
- `lr` : learning rate
- `n_epochs`: the number of epoch
- `warmup` : linearly increasing learning rate from zero to the specified value
- `save_steps` : interval for saving model
- `total_steps` : total number of steps to train

### 4. Configure Training Settings
> Set up should be done in `train.sh`
- `model_cfg` : Determine which model to use. The smaller the model is, the faster training will be.

## Run
```bash
python gendata.py
python dataprocess.py
sh train.sh
```